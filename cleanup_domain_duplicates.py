#!/usr/bin/env python3
"""
Domain Duplicate Cleanup Script
Consolidates all domain files and removes duplicates
"""

import os
from typing import Set, List

def read_domains_from_file(filepath: str) -> Set[str]:
    """Read domains from file, returning set of clean domains"""
    domains = set()
    try:
        with open(filepath, 'r') as f:
            for line in f:
                domain = line.strip().replace('www.', '').lower()
                if domain and not line.startswith('#'):
                    domains.add(domain)
    except FileNotFoundError:
        print(f"File not found: {filepath}")
    return domains

def main():
    """Consolidate all domain files and remove duplicates"""
    
    # All domain files we found
    domain_files = [
        'existing_domains.txt',
        'docs/existing_domains.txt', 
        'service_discovered_domains.txt',
        'discovered_domains_20250604.txt'
    ]
    
    print("üîç DOMAIN DUPLICATE CLEANUP")
    print("=" * 50)
    
    all_domains = set()
    file_stats = {}
    
    # Read all domains from all files
    for filepath in domain_files:
        if os.path.exists(filepath):
            domains = read_domains_from_file(filepath)
            file_stats[filepath] = len(domains)
            all_domains.update(domains)
            print(f"üìÅ {filepath}: {len(domains)} domains")
        else:
            print(f"‚ùå {filepath}: File not found")
    
    print(f"\nüìä ANALYSIS:")
    print(f"   Total unique domains: {len(all_domains)}")
    print(f"   Total from all files: {sum(file_stats.values())}")
    print(f"   Duplicates found: {sum(file_stats.values()) - len(all_domains)}")
    
    # Create consolidated master list
    sorted_domains = sorted(list(all_domains))
    
    # Write to master consolidated file
    with open('consolidated_approved_domains.txt', 'w') as f:
        f.write("# CONSOLIDATED APPROVED DOMAINS\n")
        f.write("# This file contains all unique approved domains from all sources\n")
        f.write(f"# Total domains: {len(sorted_domains)}\n")
        f.write("# Generated by cleanup_domain_duplicates.py\n")
        f.write("\n")
        for domain in sorted_domains:
            f.write(f"{domain}\n")
    
    print(f"\n‚úÖ CLEANUP COMPLETE:")
    print(f"   Created: consolidated_approved_domains.txt ({len(sorted_domains)} domains)")
    print(f"   Removed: {sum(file_stats.values()) - len(all_domains)} duplicates")
    
    # Show which files had the most domains
    print(f"\nüìà FILE BREAKDOWN:")
    for filepath, count in sorted(file_stats.items(), key=lambda x: x[1], reverse=True):
        print(f"   {filepath}: {count:,} domains")

if __name__ == "__main__":
    main()
